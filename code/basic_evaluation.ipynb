{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation \n",
    "\n",
    "In this file the evaluation scores of the systems that I trained can be found. The code computes the (macro average) precision, recall and F1 scores by comparing the outputs of the system to the gold data. All documents must be in conll format. \n",
    "The main function has an `extended` setting, which also the confusion matrix and the evaluation measures for each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# see tips & tricks on using defaultdict (remove when you do not use it)\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotations(inputfile: str, annotationcolumn: str, delimiter: str ='\\t'):\n",
    "    '''\n",
    "    This function extracts annotations represented in the conll format from a file\n",
    "    \n",
    "    :param inputfile: the path to the conll file\n",
    "    :param annotationcolumn: the name/index of the column in which the target annotation is provided\n",
    "    :param delimiter: optional parameter to overwrite the default delimiter (tab)\n",
    "\n",
    "    :returns: the annotations as a list\n",
    "    '''\n",
    "    annotations = []\n",
    "    first_line = True\n",
    "    with open(inputfile, 'r', encoding='utf8') as infile:\n",
    "        for line in infile:\n",
    "            #skip the first line in the data, this contains the column names (which equal the indices)\n",
    "            if first_line:\n",
    "                first_line = False\n",
    "                continue\n",
    "            components = line.rstrip('\\n').split(delimiter)\n",
    "            #skip empty lines\n",
    "            if len(components) > 1:\n",
    "                annotations.append(components[int(annotationcolumn)])\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_counts(gold_annotations, machine_annotations):\n",
    "    '''\n",
    "    This function compares the gold annotations to machine output\n",
    "    \n",
    "    :param goldannotations: the gold annotations\n",
    "    :param machineannotations: the output annotations of the system in question\n",
    "    :type goldannotations: the type of the object created in extract_annotations\n",
    "    :type machineannotations: the type of the object created in extract_annotations\n",
    "    \n",
    "    :returns: a countainer providing the counts for each predicted and gold class pair\n",
    "    '''\n",
    "    evaluation_counts = defaultdict(Counter)\n",
    "    for i, annotation in enumerate(gold_annotations):\n",
    "        evaluation_counts[annotation][machine_annotations[i]] += 1\n",
    "    return evaluation_counts\n",
    "        \n",
    "def safe_divide(numerator, denominator):\n",
    "    #This function divides the numerator by the denominator. If the denominator is zero it returns zero\n",
    "    try:\n",
    "        return numerator / denominator\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    \n",
    "def calculate_precision_recall_fscore(evaluation_counts):\n",
    "    '''\n",
    "    Calculate precision recall and fscore for each class and return them in a dictionary\n",
    "    \n",
    "    :param evaluation_counts: a container from which you can obtain the true positives, false positives and false negatives for each class\n",
    "    :type evaluation_counts: type of object returned by obtain_counts\n",
    "    \n",
    "    :returns: the precision, recall and f-score of each class in a container\n",
    "    '''\n",
    "    scores = {}\n",
    "    for classlabel, counts in evaluation_counts.items():\n",
    "        #True positives are the number of times we correctly classify the label\n",
    "        TP = counts[classlabel]\n",
    "        #false negatives are the number of times we should have selected the current label but selected another\n",
    "        FN = sum([count for label, count in counts.items() if label != classlabel])\n",
    "        #false positives are the number of times we should have selected another label but selected this label\n",
    "        FP = sum([label_counts[classlabel] for label, label_counts in evaluation_counts.items() if label != classlabel])\n",
    "        \n",
    "        #calculate metrics, make sure to safe divide in case the denominator is zero\n",
    "        precision = round(safe_divide(TP, (TP + FP)),3)\n",
    "        recall = round(safe_divide(TP, (TP + FN)),3)\n",
    "        F1 = round(safe_divide((2* precision * recall), (precision + recall)),3)\n",
    "        #save scores\n",
    "        scores[classlabel] = {'precision' : precision, 'recall': recall, 'f-score': F1}\n",
    "        \n",
    "    #get marco averages    \n",
    "    macro_precision = sum([score['precision'] for score in scores.values()]) / len(scores.keys())  \n",
    "    macro_recall = sum([score['recall'] for score in scores.values()]) / len(scores.keys())  \n",
    "    macro_F1 = safe_divide((2* macro_precision * macro_recall), (macro_precision + macro_recall))\n",
    "    \n",
    "    #print macro averages\n",
    "    print(f\"Macro precision score : {round(macro_precision * 100,2)}\")\n",
    "    print(f\"Macro recall score : {round(macro_recall * 100,2)}\")\n",
    "    print(f\"Macro F1 score : {round(macro_F1 *100,2)}\")\n",
    "    return scores\n",
    "\n",
    "def provide_confusion_matrix(evaluation_counts):\n",
    "    '''\n",
    "    Read in the evaluation counts and provide a confusion matrix for each class\n",
    "    \n",
    "    :param evaluation_counts: a container from which you can obtain the true positives, false positives and false negatives for each class\n",
    "    :type evaluation_counts: type of object returned by obtain_counts\n",
    "    \n",
    "    :prints out a confusion matrix\n",
    "    '''\n",
    "    #make sure all values are in the dict, and that the same order is maintained for all labels, so that we get a clean table\n",
    "    for i in evaluation_counts.keys():   \n",
    "        evaluation_counts[i] = {j:evaluation_counts[i][j] for j in evaluation_counts.keys()}\n",
    "        \n",
    "    # create matrix\n",
    "    confusions_pddf = pd.DataFrame.from_dict({i: evaluation_counts[i]\n",
    "                                              for i in evaluation_counts.keys()},\n",
    "                                             orient='index', columns=evaluation_counts.keys(),\n",
    "                                             )\n",
    "    #print matrix and latex version of matrix\n",
    "    print(confusions_pddf)\n",
    "    print(confusions_pddf.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def carry_out_evaluation(gold_annotations, systemfile, systemcolumn, extended, delimiter='\\t'):\n",
    "    '''\n",
    "    Carries out the evaluation process (from input file to calculating relevant scores)\n",
    "    \n",
    "    :param gold_annotations: list of gold annotations\n",
    "    :param systemfile: path to file with system output\n",
    "    :param systemcolumn: indication of column with relevant information\n",
    "    :param delimiter: specification of formatting of file (default delimiter set to '\\t')\n",
    "    \n",
    "    returns evaluation information for this specific system\n",
    "    '''\n",
    "    #retrieve annotations of the system\n",
    "    system_annotations = extract_annotations(systemfile, systemcolumn, delimiter)\n",
    "    #evaluate\n",
    "    evaluation_counts = obtain_counts(gold_annotations, system_annotations)\n",
    "    \n",
    "    #print confusion matrix in extended evaluation setting\n",
    "    if extended:\n",
    "        provide_confusion_matrix(evaluation_counts)\n",
    "    #get evaluation metrics\n",
    "    evaluation_outcome = calculate_precision_recall_fscore(evaluation_counts)\n",
    "    return evaluation_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_output_tables(evaluations):\n",
    "    '''\n",
    "    Create tables based on the evaluation of various systems\n",
    "    \n",
    "    :param evaluations: the outcome of evaluating one or more systems\n",
    "    '''\n",
    "    evaluations_pddf = pd.DataFrame.from_dict({(i,j): evaluations[i][j]\n",
    "                                              for i in evaluations.keys()\n",
    "                                              for j in evaluations[i].keys()},\n",
    "                                             orient='index')\n",
    "\n",
    "    print(evaluations_pddf)\n",
    "    print(evaluations_pddf.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluations(goldfile, goldcolumn, systems, extended):\n",
    "    '''\n",
    "    Carry out standard evaluation for one or more system outputs\n",
    "    \n",
    "    :param goldfile: path to file with goldstandard\n",
    "    :param goldcolumn: indicator of column in gold file where gold labels can be found\n",
    "    :param systems: required information to find and process system output\n",
    "    :type goldfile: string\n",
    "    :type goldcolumn: integer\n",
    "    :type systems: list (providing file name, information on tab with system output and system name for each element)\n",
    "    \n",
    "    :returns the evaluations for all systems\n",
    "    '''\n",
    "    evaluations = {}\n",
    "    #extract gold annotations\n",
    "    gold_annotations = extract_annotations(goldfile, goldcolumn)\n",
    "    #evalutate\n",
    "    for system in systems:\n",
    "        sys_evaluation = carry_out_evaluation(gold_annotations, system[0], system[1], extended)\n",
    "        evaluations[system[2]] = sys_evaluation\n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the overall set-up\n",
    "\n",
    "The functions below illustrate how to run the setup as outlined above using a main function and, later, commandline arguments. This setup will facilitate the transformation to an experimental setup that no longer makes use of notebooks, that you will submit later on. There are also some functions that can be used to test your implementation You can carry out a few small tests yourself with the data provided in the data/ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_evaluation_value(system, class_label, value_name, evaluations):\n",
    "    '''\n",
    "    Return the outcome of a specific value of the evaluation\n",
    "    \n",
    "    :param system: the name of the system\n",
    "    :param class_label: the name of the class for which the value should be returned\n",
    "    :param value_name: the name of the score that is returned\n",
    "    :param evaluations: the overview of evaluations\n",
    "    \n",
    "    :returns the requested value\n",
    "    '''\n",
    "    return evaluations[system][class_label][value_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_information(system_information):\n",
    "    '''\n",
    "    Takes system information in the form that it is passed on through sys.argv or via a settingsfile\n",
    "    and returns a list of elements specifying all the needed information on each system output file to carry out the evaluation.\n",
    "    \n",
    "    :param system_information is the input as from a commandline or an input file\n",
    "    '''\n",
    "    systems_list = [system_information[i:i + 3] for i in range(0, len(system_information), 3)]\n",
    "    return systems_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main(my_args=None, extended=False):\n",
    "    '''\n",
    "    A main function. this makes sure to carry out the evaluation for the given gold and machine annotation file\n",
    "    \n",
    "    sys.argv is a very lightweight way of passing arguments from the commandline to a script.\n",
    "    \n",
    "    :param my_arg : a list containing the following parameters:\n",
    "                    args[0] : the path (str) to the goldfile\n",
    "                    args[1] : the index of the column in the gold file in which the gold labels can be found\n",
    "                    args[2] : the path (str) to the file containing machine annotations to be evaluated\n",
    "                    args[3] : the index of the column in the machine annotations file in which the annotations can be found\n",
    "                    \n",
    "    :param extendend: if this is true, the output is not only the performance measures but also the confusion matrix and \n",
    "                      the scores per label. Default is false \n",
    "    '''\n",
    "    if my_args is None:\n",
    "        my_args = sys.argv\n",
    "        \n",
    "    \n",
    "    system_info = create_system_information(my_args[2:])\n",
    "    evaluations = run_evaluations(my_args[0], my_args[1], system_info, extended)\n",
    "    #in the extended setting, also print confusion and output tables\n",
    "    if extended:\n",
    "        provide_output_tables(evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations\n",
    "Below, the results for all models I evaluated can be found\n",
    "\n",
    "### pre-trained models\n",
    "Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 53.86\n",
      "Macro recall score : 62.77\n",
      "Macro F1 score : 57.97\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed.conll','3','../data/spacy_out.dev-preprocessed.conll','2','system1']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 34.33\n",
      "Macro recall score : 49.16\n",
      "Macro F1 score : 40.43\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed.conll','3','../data/stanford_out.dev-preprocessed.conll','3','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering models\n",
    "#### Logistic Regression\n",
    "basic setting (token only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 81.13\n",
      "Macro recall score : 54.76\n",
      "Macro F1 score : 65.38\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/logreg_basic.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extended setting (all features included, token as one-hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 85.97\n",
      "Macro recall score : 78.98\n",
      "Macro F1 score : 82.32\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/logreg_extended.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings setting (all features included, token as word embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 81.21\n",
      "Macro recall score : 77.6\n",
      "Macro F1 score : 79.36\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/logreg_embeddings.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM\n",
    "basic setting (token only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 80.57\n",
      "Macro recall score : 64.99\n",
      "Macro F1 score : 71.94\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/SVM_basic.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extended setting (all features included, token as one-hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            O  B-ORG  B-LOC  B-MISC  I-MISC  B-PER  I-PER  I-LOC  I-ORG\n",
      "O       42606     26     12      21      14     38      9      2     31\n",
      "B-ORG      49   1041    100      24       1    100      4      1     21\n",
      "B-LOC      59     87   1604      23       0     46     10      4      4\n",
      "B-MISC     54     55     40     736       5     25      2      0      5\n",
      "I-MISC     41     13      4      13     225      5     26      8     11\n",
      "B-PER      58     28     41      12       0   1665     32      0      6\n",
      "I-PER      25      2      3       2       1     15   1251      3      5\n",
      "I-LOC      12      2      5       0       2      1     17    200     18\n",
      "I-ORG      79      6     23       7      15      7     63     38    513\n",
      "\\begin{tabular}{lrrrrrrrrr}\n",
      "\\toprule\n",
      "{} &      O &  B-ORG &  B-LOC &  B-MISC &  I-MISC &  B-PER &  I-PER &  I-LOC &  I-ORG \\\\\n",
      "\\midrule\n",
      "O      &  42606 &     26 &     12 &      21 &      14 &     38 &      9 &      2 &     31 \\\\\n",
      "B-ORG  &     49 &   1041 &    100 &      24 &       1 &    100 &      4 &      1 &     21 \\\\\n",
      "B-LOC  &     59 &     87 &   1604 &      23 &       0 &     46 &     10 &      4 &      4 \\\\\n",
      "B-MISC &     54 &     55 &     40 &     736 &       5 &     25 &      2 &      0 &      5 \\\\\n",
      "I-MISC &     41 &     13 &      4 &      13 &     225 &      5 &     26 &      8 &     11 \\\\\n",
      "B-PER  &     58 &     28 &     41 &      12 &       0 &   1665 &     32 &      0 &      6 \\\\\n",
      "I-PER  &     25 &      2 &      3 &       2 &       1 &     15 &   1251 &      3 &      5 \\\\\n",
      "I-LOC  &     12 &      2 &      5 &       0 &       2 &      1 &     17 &    200 &     18 \\\\\n",
      "I-ORG  &     79 &      6 &     23 &       7 &      15 &      7 &     63 &     38 &    513 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Macro precision score : 86.71\n",
      "Macro recall score : 82.39\n",
      "Macro F1 score : 84.49\n",
      "                precision  recall  f-score\n",
      "system2 O           0.991   0.996    0.993\n",
      "        B-ORG       0.826   0.776    0.800\n",
      "        B-LOC       0.876   0.873    0.874\n",
      "        B-MISC      0.878   0.798    0.836\n",
      "        I-MISC      0.856   0.650    0.739\n",
      "        B-PER       0.875   0.904    0.889\n",
      "        I-PER       0.885   0.957    0.920\n",
      "        I-LOC       0.781   0.778    0.779\n",
      "        I-ORG       0.836   0.683    0.752\n",
      "\\begin{tabular}{llrrr}\n",
      "\\toprule\n",
      "        &       &  precision &  recall &  f-score \\\\\n",
      "\\midrule\n",
      "system2 & O &      0.991 &   0.996 &    0.993 \\\\\n",
      "        & B-ORG &      0.826 &   0.776 &    0.800 \\\\\n",
      "        & B-LOC &      0.876 &   0.873 &    0.874 \\\\\n",
      "        & B-MISC &      0.878 &   0.798 &    0.836 \\\\\n",
      "        & I-MISC &      0.856 &   0.650 &    0.739 \\\\\n",
      "        & B-PER &      0.875 &   0.904 &    0.889 \\\\\n",
      "        & I-PER &      0.885 &   0.957 &    0.920 \\\\\n",
      "        & I-LOC &      0.781 &   0.778 &    0.779 \\\\\n",
      "        & I-ORG &      0.836 &   0.683 &    0.752 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/SVM_extended.conll','9','system2']\n",
    "main(my_args, extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings setting (all features included, token as word embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 81.13\n",
      "Macro recall score : 75.67\n",
      "Macro F1 score : 78.3\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/SVM_embeddings.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "basic setting (token only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 78.71\n",
      "Macro recall score : 64.73\n",
      "Macro F1 score : 71.04\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/NB_basic.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extended setting (all features included, token as one-hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 76.66\n",
      "Macro recall score : 75.93\n",
      "Macro F1 score : 76.29\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/NB_extended.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 85.52\n",
      "Macro recall score : 80.7\n",
      "Macro F1 score : 83.04\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev.conll','3','../data/CRF.conll','2','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for ablation study (SVM - extended setting)\n",
    "including ALL features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 70.49\n",
      "Macro recall score : 65.69\n",
      "Macro F1 score : 68.0\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/SVM_extended_noToken.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No POS:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 86.16\n",
      "Macro recall score : 82.39\n",
      "Macro F1 score : 84.23\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/SVM_extended_noPos.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Cap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 86.61\n",
      "Macro recall score : 81.43\n",
      "Macro F1 score : 83.94\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/SVM_extended_noCap.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 86.64\n",
      "Macro recall score : 82.34\n",
      "Macro F1 score : 84.44\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/SVM_extended_noNumber.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 86.67\n",
      "Macro recall score : 82.34\n",
      "Macro F1 score : 84.45\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/SVM_extended_noPunct.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Previous token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 84.47\n",
      "Macro recall score : 77.47\n",
      "Macro F1 score : 80.82\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/SVM_extended_noPrevToken.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Previous POS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 85.73\n",
      "Macro recall score : 81.53\n",
      "Macro F1 score : 83.58\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/SVM_extended_noPrevPOS.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Token and no PrevToken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 43.78\n",
      "Macro recall score : 44.26\n",
      "Macro F1 score : 44.02\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/SVM_extended_noTokenPrevToken.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Token and no PrevToken, and no POS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 43.13\n",
      "Macro recall score : 36.77\n",
      "Macro F1 score : 39.7\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/SVM_extended_noTokenPrevToken_Pos.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Token and no PrevToken, and no Capitalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 41.1\n",
      "Macro recall score : 38.41\n",
      "Macro F1 score : 39.71\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/SVM_extended_noTokenPrevToken_Cap.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Token and no PrevToken, and no number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 44.19\n",
      "Macro recall score : 43.61\n",
      "Macro F1 score : 43.9\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/SVM_extended_noTokenPrevToken_Number.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Token and no PrevToken, and no punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 44.0\n",
      "Macro recall score : 43.6\n",
      "Macro F1 score : 43.8\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/SVM_extended_noTokenPrevToken_Punct.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Token and no PrevToken, and no previous POS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 38.1\n",
      "Macro recall score : 27.13\n",
      "Macro F1 score : 31.69\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/SVM_extended_noTokenPrevToken_PrevPos.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights:\n",
    "* The token is the most important, previous token follows\n",
    "* if we exclude these both, we see that the previous POS becomes the most important, while this feature barely had an effect when the token and the previous token were still included\n",
    "* if we exclude these both, POS and capitalization also have an effect\n",
    "* if we exclude these both, Number and Punctuation still do not contribute\n",
    "\n",
    "Are the same features important for logistic regression? Let's see..\n",
    "\n",
    "No token embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 71.57\n",
      "Macro recall score : 63.93\n",
      "Macro F1 score : 67.53\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/logreg_extended_noToken.conll','9','system2']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No previous token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision score : 82.16\n",
      "Macro recall score : 74.47\n",
      "Macro F1 score : 78.12\n"
     ]
    }
   ],
   "source": [
    "my_args = ['../data/conll2003.dev-preprocessed-features.conll','3','../data/logreg_extended_noPrevToken.conll','9','system2']\n",
    "main(my_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
